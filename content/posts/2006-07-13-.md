---
title: 'RailsConf 2006 Talk Transcript: The BBCâ€™s Programme Catalogue on Rails'
author: Matt Biddulph
type: post
date: 2006-07-13T21:28:05+00:00
excerpt: \n
draft: true
url: /?p=90
categories:
  - Uncategorized

---
This is an edited transcript of the talk I gave at the Rails conference in Chicago a few weeks ago. You might want to read along with [the slides][1].

<!--more-->

### Introduction

Thank you all for coming today. It&#8217;s very nice to be here, and nice to be representing the British side of the global Rails community. I&#8217;m here to tell you about how Rails was used at the BBC to put together a pretty interesting website. I&#8217;ll try to give an insight into how we built the thing, the results, give you a bit of a demo, and talk about how Rails worked for us.

So, who here doesn&#8217;t know what the BBC is? Everyone? Good. Who knows who I am? One person? Thank you.

<!-- laughter -->

Ok, so my name&#8217;s Matt Biddulph. I&#8217;m an independent freelancer &#8211; just a contractor. I only built this thing. I don&#8217;t work at the BBC.

<!-- laughter -->

I worked at the BBC for about four years. I worked in interactive television and in radio. I&#8217;m sure many of you have &#8220;Listened Again&#8221; to BBC radio programmes over the internet. I used to work in a team that built that stuff. We built interactive systems for television. We built text messaging services for radio. But these days I&#8217;m no longer a manager, and back to hands-on coding, which is a really good thing.

So BBC has this long, long history. It goes back at least 80 years, and it has a really good sense of that history. You can find all sorts of stuff about it online. When googling for BBC pictures you can find these great 1930s studios with fantastic dials and men with pipes. It&#8217;s got this big long history but it&#8217;s still making shows and it&#8217;s got an important place in British culture. I don&#8217;t know how many of you know for example &#8220;The Office&#8221;, a fantastic hit in the UK, a great comedy which even had an American remake recently. I&#8217;m sure everyone knows Dr Who.

Rails, on the other hand, is still quite young.

<!-- laughter -->

The project started with the database of the BBC&#8217;s archive. If you go out to the Northwest edge of London, an area called Windmill Road, you&#8217;ll find a warehouse that was built in the 1960s. The roof leaks, it&#8217;s made of corrugated iron, and there are miles and miles and miles of shelving. They have recordings in every format with examples of pretty much everything the BBC has ever made. The oldest one I could find was from around 1937 and it was a sound recording of a poetry reading.

This archive goes back through the history of formats. For example, the official archive format for BBC radio up until the mid &#8217;80s was actually vinyl. Whenever they wanted to archive a programme they just cut a 12-inch down in the basement of broadcasting house in the middle of London and someone took it over by motorbike to the warehouse.

In this picture is Tom Loosemore &#8211; he&#8217;s the guy who commissioned this project, and the project director of BBC 2.0. This project is all part of the BBC opening up access to its programmes and information. What we have there is a stack of 70s 2-inch videotape that&#8217;s being thrown out after conversion to DVD. Each one stores around ninety minutes of broadcast quality video, and used to cost (in modern money) about 3000 pounds.

They&#8217;ve got a rolling programme of updating this stuff, taking all this archival footage and moving it onto new formats: MPEG-4, DVD, etc. I was allowed to actually walk around the archive. It&#8217;s fantastic for a media geek like me; someone who loves old TV and old radio. Some of the archive footage is physically deteriorating. It needs copying onto new media before it turns into vapour. They have archivists there who, for years and years, going right back to index cards and typewriters, have been indexing this stuff. They need to know what&#8217;s on these tapes because the BBC is constantly re-using its old footage in news and in new productions.

So we got this great project. They just said &#8220;you can have this database: a 2GB dump of pipe-delimited plaintext. Make it web 2.0.&#8221;

### Demo

Let&#8217;s see if I can break out of here and give you a demo. So this stuff goes back a really long time. I&#8217;ve got a bunch of canned searches just in case it doesn&#8217;t work. &#8220;Dr Who&#8221; for example. Dr Who: 751 programmes, going back to 1963. Any number of other television programmes about &#8220;Dr Who&#8221;, lots of &#8216;subject categories&#8217;, which is a nice traditional British librarian word for &#8216;tags&#8217;.

<!-- laughter -->

And this is not a beta, it&#8217;s an _experimental prototype_: two of the uniquely English spelled words in the English language.

Basically this is like an [IMDB][2] for the BBC. It&#8217;s entirely running on Ruby on Rails. It was built from scratch in about two months from start to finish.

It&#8217;s got a great search engine. You can see the schedule for any day in history. You can go and see exactly what was shown on New Year&#8217;s Eve 1999, for the Millennium.

Say you&#8217;re interested in Jon Pertwee, one of the original Dr Whos from way back. We have all this metadata about him: first broadcast, last broadcast, a cute little sparkline of his broadcast history. What else did he do? Actually he wasn&#8217;t just Dr Who, he was in loads of children&#8217;s programmes before and after &#8220;Dr Who&#8221; &#8211; a much loved actor. Who did he often appear with? And this is a combination of stuff that was recorded by the librarians into this archive, and stuff that we derived in a second order way by analysing the data.

When you got a great big database with all these human efforts gone into it, often you can find amazing pattern just by doing bit of extra second order system analysis. We have a list of who appears in what programme, so we run through it in a batch process and find out who they most often appear with. There&#8217;s a tag cloud so you can see that John Pertwee is mostly associated with science fiction, broadcasting, travel and exploration, and Brigadier Lethbridge-Stewart.

But if you want to go back and to say, the news on the 20th of November in 1985 then the level of information about that is really deep. We&#8217;ve go all these inline links so if you want to know who Graeme Souness is &#8212; he was captain at Liverpool back in 1985 &#8212; what the news was about on that day, there are little links on the right. There&#8217;s a broadcast history, so for a programme that gets repeated you can see how many times it was on.

And what&#8217;s this down here? Every single page has an RDF/XML data file that is licensed for non-commercial use. Every piece of data has an XML version, and in fact every list of data, that&#8217;s every search result, every list of contributions by a particular actor, has an Atom feed. So it&#8217;s a really, really open sort of website. It&#8217;s there for people to use it.

### From green-screen to the web

This is native Infax. Infax is the librarians&#8217; database application. They had a massive project in the &#8217;80s to take a huge room full of typed index cards and translate them to a relational database with a text interface. This is what we had before the web: web -1.0.

It&#8217;s all very manual and key-controlled. For example, to classify something with a tag, you type in a sort of Dewey-decimal number. Except they&#8217;re about ten digits long and they&#8217;ve got dots all over them. Every day the chief librarian goes and looks through a printout of the changes and actually corrects it with a pen.

The thing is, this was actually really a rigorous bit of data modeling. They were able to give me a nice big wad of documentation. Everything is very carefully controlled; everything has a unique ID. It&#8217;s actually pretty suitable for web use. It&#8217;s the kind of thing that lends itself to URLs that actually mean something: a URL for every concept. All great Library Science.

The taxonomy that they use for categorisation is actually much richer than what you can see on the website. It&#8217;s a big hierarchical taxonomy of all the concepts in the world that have been important enough to feature on TV and radio. And it&#8217;s constantly being updated. For ease of use, we decided to boil it down to a simpler, tag-like view for the first version of the web site.

The chief librarian comes into work every day (when she&#8217;s not fighting vampires), and there&#8217;s a printout from the previous day that&#8217;s about three inches thick waiting on her desk. She goes through it with pen and makes sure no one&#8217;s created a new taxonomic term when they could&#8217;ve re-used a previous one. So you get incredible precisision and a great lookup on, say, the Afghanistan situation in the 1980s or the coming of the Euro to Europe.

We&#8217;re going to move from this relational model and this blue screen thing where there&#8217;re no hyperlinks. The librarians in some ways don&#8217;t know what they&#8217;ve got in that age of application because they can&#8217;t click around it in the way that we&#8217;re used to, browsing with a web browser. You can&#8217;t actually open up the data model very directly because you&#8217;ve got this blue screen up. Everything&#8217;s just a case of: search for this tag, search for this thing, view one screen of information at the time.

In web design, I think it matters very much, I can&#8217;t recommend it enough, it took Tom Coates of Yahoo, good native to a web of data. He&#8217;s been getting at several conferences this year. And the thrust of this talk is that, designing for the data web, designing essentially for APIs, for web services, for open data, leads to great websites. If your websites are about information, then thinking about the data and appropriate identifiers for that data, URLs, so that people can link to your web of data, extend out your web of data, is an amazingly powerful thing. It&#8217;s the difference between being on the web and being part of the web.

So I&#8217;d like to think these URLs here is part of the site. Every URL on the site is unique, it&#8217;s indicative of one concept. So the top programme there, is a particular programme. The second URL is the XML version of that programme&#8217;s information. The third one down, is anything tagged as ponies. The fourth thing down, is everything that was on the day I was born. Fifth thing down is search for my surname. Sixth is the one time that my dad, who is a local council representative spokesman guy, was on the local news. An indication of the depth of this thing.

We get nightly updates from this database, and so when I launch it I go &#8220;wow this is so deep my dad is on there&#8221;, and then during the three months we&#8217;ve been running it, my grandpa turned up on there because he&#8217;s been on a local news thing, and my grandma, and my ex-girlfriend. My website&#8217;s stalking you.

<!-- laughter -->

So you do nice simple URL things, guessable, hackable, they look like the kind of thing you could take and use as an identifier to the system. They&#8217;re almost like foreign keys for the web. Don&#8217;t know how many of you know Ben Housely. This comes off much better in Europe. How did I work? This whole website was built by two people, me and this guy Ben Housely. And as you can see you don&#8217;t want to meet up with this guy too often.

So here in Florence, Italy, I was able to say to him, essentially you only get to touch, hopefully the CSS but certainly only allowed in app/views. And actually that worked incredibly well, so I was able to build the entire rails app. If he needed some new macro then I&#8217;d write him a little bit of an extra helper. I wrote the initial cut of the website in drop dead simple xHTML, just sort of strict xHTML, basically it classes all of the things that looked like they were meant to be grouped together.

And Ben was able to just come in there, I think the amount of changes he was making we didn&#8217;t even end up having to give him SVN commit access. He would just throw me new files every so often. The changes were that small, and that I found a really, really powerful thing. The site has this gettable API. In some ways it&#8217;s not obvious the site has an API. We don&#8217;t have an API page. It&#8217;s not an RPC style things where we say &#8220;here are the things that our website will do for you if you call these endpoints&#8221;. It&#8217;s simply that every piece of information on the website has a unique URL which if you request it will give you a machine readable version.

So we put up this information, great big loads of XML, some people care to dive into it, and within a few days we had this springy touch graph thing. So this is a map of Tony Blair and John Major in the center there, two British prime ministers. That was a stretchy map thing you pull about and click on things that would show you who most commonly appear with other people. So you can work out which are the cliques, with journalist got to interview which politicians, which politicians tended to appear on the same programmes. And start to build a kind of semantic picture of what&#8217;s been going on in British politics.

We then turn to Wikipedia. Wikipedia is absolutely fantastic. It&#8217;s another source of first class things. There&#8217;s a Wikipedia page for pretty much everything that somebody in the world cares about. It turns out that it really does have a great search API, it&#8217;s just that it&#8217;s developed at Yahoo.com.

<!-- laughter -->

This I actually found a great method on a few projects recently. Basically it&#8217;s what people have been doing on blogs for years, which is put a &#8220;search my blog&#8221; with Google button on the site, but actually using the other API, which I prefer because it&#8217;s to the Google one, which I prefer for its web browsing experience. You just plug in site restriction en.wikipedia.org, and then you&#8217;ve search it, and then you&#8217;ve got a search API for Wikipedia. And actually Wikipedia&#8217;s does have great XML dumps of the address that&#8217;s in there.

So the Rails aspect. We built this site just shy of 4000 lines of code, that includes HTML templates, and includes all the Javascript, includes all the Ruby code. And I really found that this is not a really complicated website.

The big deal with this website was getting a bulk of data online, which I say dates back 80 years, covers nearly a million programmes. The total of the number rows in the database of the engine is about seven million, so a couple of Gigabytes of plain text is quite a lot to index. Any one page is quite simple to build, but getting the whole website together, and getting it to serve up traffic, the kind of speed the BBC can get hit by was the really interesting challenge. So I really found this sort of golden path concept, the simplest thing you want to do is take the fewest lines of code, absolutely worked from this application.

The trickiest thing is getting the database stuff right. And I&#8217;ll talk a little bit later about performance. We went through some iterations on the database to get it to perform better. The initial approach was to take this great mixed database. It wasn&#8217;t enormous by some corporate standards. It was only about 25 tables, just lots and lots and lots of rows, and some really weird encoding for some of the fields. I looked at it and I thought, firstly this is not going to fit well with ActiveRecord using customizations. It&#8217;s going to be fiddly. Secondly this is not a read optimized skimmer.

I think that&#8217;s actually my biggest bit of advice for people working with legacy databases, is whether or not you can adapt to their structure. It may well be that for example, to resolve any one piece of information from a nice simple web page, you might just need to do too many joins. Good read-write databases have a lot of lookup tables, have a lot of things in there for consistency, and they&#8217;re not optimized for &#8220;tell me these two things I need to know&#8221; generally.

So actually I went to a great talk at Euro OSCon last year by the guy from O&#8217;Reilly who does their data warehousing. I really recommend if you get the chance to see him talk on that subject. He does a talk on building open source data warehouses. So just taking MySQL on Pearl, Python, picking up a bunch of stone tools and just using really intelligent schemas, sort of inspired from the data warehousing world rather than greatly products that do data warehousing, and use them to sort of get good performance out of a maximum amount of data query.

Particularly, I think the star schema is really inspiring, essentially just that you, if you have a website or a whole bunch of data who&#8217;s central concept is very clear, so in this case the programme, everything revolves around the programme, the BBC model. Then putting that on the center of the star of your schema, so that everything you can get from that programme, to any other thing with a single join, really, really speeds up your on-the-fly queries. Because any given query, the length of the join chain is only one, and the number of joins going away it is very low.

There&#8217;s not only really obvious Ajax on the site, there&#8217;s a bit of DHTML to move some things around as part of the user experience. The trickiest is, I found this data is, everything you want to take a broad view of the stage like this. So as I said, BBC goes back to 1937 and has been updated every day. So sometimes you want to look at something like say all the &#8220;Dr Who&#8221; programmes ever. And you&#8217;re going to pull back a lot of data from a lot of the places. But actually you can obviously pull back summary queries, you can do select group by sort of stuff.

So what we did with quite a few of the pages, particularly this big list of every programme in the series was we did the summary query, displayed essentially the summary result with tree expander things. And only when you hit the tree expander did it go back via Ajax and perform the drill down query, and just for that one year. So we saved a lot of the over-the-top database activity with that.

The other big issue I had to tackle was how do you do this transformation? How do you take this legacy database? You&#8217;ve worked out &#8220;Ok, here&#8217;s my new structure, here&#8217;s how many needs to be transformed, tables I need to throw away and turn into specific fields, all this kind of stuff.&#8221; There&#8217;re a series of jobs you have to perform there, to important programme, then you pull the contributors to that programme and make part to the programme, you have this data in your ActiveRecord friendly schema.

So what I ended up doing was writing a low priority queue job processor. So SQL is not the nicest thing in the world to create a priority queue structure in, but it pretty much works as long as you don&#8217;t have the table filled up too much. I used single table inheritance to create a job super-class, with a bunch of descriptive fields like the primary key and the table the job was going to operate on. Each type of job has a priority which ends up in plain sort of flow-chart operations that it can run through.

And then your simple query, every time you want to pull a job off the stack and do it, is find me the highest priority job that isn&#8217;t done yet, set it&#8217;s status from ready to process it, do it, then set its status to done or downFL then carry on. It&#8217;s actually an easy distributable if your operations are somewhat in plans of each other, you could run several job processes on several machines because actually most of the processing time is Ruby time there because you are doing things like transforming strings, transforming author names and so forth.

So that turned out to be quite a nice structure, and then I could stay inside the nice, safe world of ActiveRecord semantics and not drop through to some separate processor, because obviously the other two major approaches with legacy data are create an ActiveRecord configuration or a views configuration within your database, which makes it look ok to the code. Or work through these transformation steps. And the transformation steps you have those two, you can either do it within ActiveRecord or you can probably do it much faster in custom code or custom SQL. But it does increase the maintenance problems with your code there.

So search is a massive part of this application. The information&#8217;s just there for people to find stuff out about programming. So, I have found over the years actually it&#8217;s really hard to find good full text search particularly in the open-source world. And it&#8217;s only in recent years that two decent sort of large scale applets have come up. One of them is Lucene for Java, a phenomenal open-source free text search engine. Finally actually an index which you insert or delete on the fly, has fields, which had generally not been the case.

But again, staying within the nascent world trying to reduce the number of variables, not flitting through the job and not using a port of Lucene, staying entirely within this Rails application. As you might find MySQL has gotten better and better over the years with full text search. The most important thing is to turn off the dumb defaults. Now in MySQL, there is this great free text index people do. It&#8217;s pretty fast, it&#8217;s good for small to medium to fairly big applications. But by default it has a built-in stop list, which means you will never be able to find the word &#8220;the&#8221; in any data. And it only indexes words that are four characters or longer. So you saw the search for &#8220;Dr. Who&#8221;, it just didn&#8217;t work when I first did an index search.

(laughter).

It&#8217;s really fiddley. It&#8217;s actually a global configuration thing, it&#8217;s in your &#8220;my.conf&#8221; and you have to restart the MySQL server to do it and you have to then drop the index, if you&#8217;ve previously created the index with these words missing.

But once you&#8217;ve actually done this it&#8217;s pretty good. It has two modes for the searches it can do. It has a sort of built-in syntax that you can pass in that gives you a score back, a scoring matrix. But they are a little bit opaque and they are certainly not tuned for your application how you want to prioritize data coming back. So what I did was I used, you add the phrase &#8220;in Boolean mode&#8221; to your search string in SQL, which switches to another mode which is much more precise. It&#8217;s a little bit like the Google &#8220;add plus&#8221; on the side of your search terms to say it&#8217;s mandatory or optional, that sort of thing.

And then I wrote a little query pass on the front end within the Rails app, broke down the search query, applied a few little extra things to kind of get a Google style feel good stuff where you can do, I don&#8217;t know title code on title fields, that sort of thing. And boils it down to these Boolean mode things, and then wrote my own scoring applet, and so I looked at what comes back, and I think the programme is more important than the contributors, unless the contributor vein exactly matches the string that you first searched for. And we pull out these things we call best bets on the top of that list.

That&#8217;s one of the hardest things, I&#8217;ve found that everywhere I&#8217;ve ever made an included search is how do you make search that feels intuitive to the user? How do you make them feel like they are finding the good stuff and not missing out on something? And I think that you know that&#8217;s really Google advantage these days even if it&#8217;s only in our heads.

So, performance, it&#8217;s like giving a good performance. I love Flickr. Who needs stop photography these days?

So, we opened up this website a couple months back. We actually didn&#8217;t, the BBC can fire an enormous amount of traffic at a website. If you get on the BBC home page you can expect 10 million hits in the first day at the least. Or if you get an announcement on live radio, you just sort of get a burst of attention. It&#8217;s like Slashdot-ing probably was once, cause Slashdot used to be good.

<!-- laughter -->

So, what we did, we were a bit scared, we happened to be the BBC&#8217;s first Rails App. We benchmarked it, we knew we had a good mix of URLs, it was a pretty good performer. But we knew it was a very, very broad data set, so our caching was not necessarily going to be as effective&#8230;it&#8217;s a classic long tail app, it is. This is about what are you interested in? It&#8217;s right down there in the BBC&#8217;s archive. So we benchmarked it, we deployed it on a single core CPU, Dell Linux rack machine running an old version of Red Hat enterprise Linux. So really nothing too special.

So, in a good mixture of URLs hitting us with a benchmarking test it could theoretically serve 30 requests per second. And over the first four days I went in to check the production log, just to check the timers for everything that had been hit with so far. So even discounting the static file cache, the caches page stuff that the Apache was handling for me, 96.5% of all the requests were completely fulfilled in under a second. So, actually the average was 37 milliseconds per request so it was well under that required for a really, really snappy experience. Also because we used really nice, clean simple, small xHTML, the browser just renders it in split seconds. So if you were in London or New York, where the other BBC data center is, and you where clicking around this web site, it really felt that it was running on your own laptop. It was blazingly fast, just felt great.

But I am not going to talk much about scanning, because Karl Henderson knows more than just about anyone in the world about scanning websites. The guy who wrote Flickr, his book just came out the other day, I&#8217;ve been reading it in bits during the conference. It is absolute dynamite. It has a massive chapter about things like scanning into MySQL, federating data, how to build API&#8217;s that, where people still can&#8217;t take your website down, so I won&#8217;t talk to much about the scaling stuff.

But the way we load tested I think is kind of interesting, the most important thing, I think. When you have got a site that is ready to go is to test it on realistic URL&#8217;s. It is great to optimize during your testing phase, and when you are working on any particular new feature, but in the end you are just going to have to work with a realistic mix of URL&#8217;s, because that is what the user is going to hit it with. So I use this neat bit of software called &#8216;Siege&#8217;, which I haven&#8217;t seen mentioned too often. And apparently HTTPperf from HP labs is another good one for this.

Essentially what you want is software that will take a big file, let&#8217;s say sixty thousand URL&#8217;s in a big text file, will randomize them and then hit your server from as many threads as you specify with realistic human style patterns. You would hit, back off, hit, back off, randomize delays between things, and that gives you a nice measurement per second that you can realistically expect to get from your server.

And the nice thing you can do with Rails, of course, because you have the reverse routing, because of URL4, it is very, very easy to generate a randomized list of a great number of possible URL&#8217;s for your website. Essentially just run through all your models plugging in as many perms as you can into your routes, run them backwards, dump that out as a text file and feed that into one of these performance testers. So yeah, job done, went snowboarding.

<!-- laughter -->

It&#8217;s not me, I did take the picture, though, pleased with that one. That is me, not quite as good. I forgot about something. I&#8217;m an independent contractor. I&#8217;m sure a lot of people in this room know this pain. You bust a gut, you create something you are really proud of, you put this website together, you burn the gold master, you take it into the client, it goes off to the hosting people, it&#8217;s a little snotty.

Okay, so we got this up two months, and then took a further four months to deploy. So the BBC outsourced their hosting, and this I think is the biggest problem. BBC, plenty of goodwill to try out this new technology, they were really please with the fast turnaround of the productivity of the project itself. The thing is, when you&#8217;ve got a pretty low budget, the BBC has a fixed income. It does not profit from anything, profits from almost none of its activities apart from selling DVD&#8217;s. So every extra person that hits their website is basically sucking away from their budget that pays for programming, lets them employ cleaners, pay social security and all this kind of stuff.

So having a pretty tight budget they have a very locked down process. They support essentially just one technology in the datacenter, which is Perl. They are pretty good now at managing mod-Perl servers, and so what we ended up with was not the corporate data-center in New York and London, with at least 40 servers in the web serving cluster. We ended up with, as I said, a single Dell server. It was kind of, it wasn&#8217;t actually under someone&#8217;s desk, but it was like the official SLA equivalent of that sort of stuff, and essentially it was a crash and burn server. It was were people were allowed to, maybe just about allowed to run new technologies. So we didn&#8217;t end up with all the redundancy we would normally get from this stuff. Particularly as a freelancer, and that is a really big consideration, going to a company like that.

We didn&#8217;t have the official standard server monitor, that the BBC employs across all its servers for everything else. We didn&#8217;t have control over our configuration. We couldn&#8217;t choose GEMS, for example. So, the BBC standardizes on Red Hat Enterprise Linux. If there is anything that isn&#8217;t in Redhat Enterprise 3, which I think is a now a couple of years old, they build RPM&#8217;s from scratch. So essentially, the first thing they did when they saw Rails was start pulling the GEMS apart again and start building RPM&#8217;s out of them.

That was kind of fine except we started to build this back in November, December last year. 0.13.1 was the version we built on so by the time we launched we were nearly on 1.1, but the RPM&#8217;s they built we still at 0.13.1. So I had a lot trouble working out, when things went wrong it was hard to ask for support because quite fairly everyone was like, &#8220;Why are your running 0.13.1 and that has got this bug and that problem? Just upgrade and you will be fine.&#8221; Right, I can but I don&#8217;t have a staging server, except my laptop and it isn&#8217;t running Red Hat Linux.

So you have got to be very careful to have all the stuff a developer needs when deploying a website, proper monitoring, some influence over the configuration, and a place to stage your application. And they only serve on Apache, so we had to go with multipath CGI, just bluh. I have got really, really unhappy with fastCGI in the past six months. I just discovered MONGREL in the past week or so, it&#8217;s fantastic. No but fastCGI is just nasty. Because of the lack of server monitoring I didn&#8217;t know that every night that when they hopped Apache to rotate the locks, all my dispatched FCGI&#8217;s were dying off. I had the opposite problem with the Dreamhost virtual hosting that we heard about yesterday, where some of my questions being killed off, they weren&#8217;t being left there because I&#8217;m on a crash and burn server. When they finally checked in they were like, &#8220;Do you need 167 of these processes?&#8221;

&#8220;Um, no. There should be eight.&#8221;

<!-- laughter -->

So yeah, what would happen was the classic Linux thing? They grew and grew and grew. So then they killed off, the schedulist starts randomly reading processes. And sooner or later, you know, because there were all of these things and mostly these are the ones getting randomly read. Sooner or later, there&#8217;s a little process called MySQL, it&#8217;s really essential to the site.

And so yeah, these kinds of problems, you really need to be able to know what&#8217;s going on in your server, and you really want when this happens to have some understanding of what processes are that run these things.

So in conclusion, I&#8217;m immensely proud of this project, because I love the information that&#8217;s in there. The BBC is proving to be a part of the UK&#8217;s culture. We have this great sort of explosion of blogging, of course, about science, as soon as people started using it. Wow, BBC&#8217;s using Rails. Wow, BBC&#8217;s opening up, open date on the Internet.

Within a week or two that stuff will die down; it went fast, something for your kids to go look at instead. And all the blogging, all the stuff I was finding on track backs were people just using this as the blue underline, using us to be the reference for a concept. So that you&#8217;re talking about a programme they saw in the seventies, or a political movement in the eighties, or a news radio in the nineties, and without mentioning the BBC they would just casually link to one of our pages. And that, I think, is one of the most important and interesting things you can do with a website. As I said, to be part of the web, not simply sort of on the web over there.

And yeah, I think on the Web 2.0 we did pretty well on the data side of things. It&#8217;s a classic big how the data is the internal inside there. It&#8217;s kind of hackable, you can do stuff with it. It is a perpetual experimental prototype, and you have to write a remix. Usually isn&#8217;t rich as in beautiful and pleasurable but it&#8217;s actually full of information, and the informational architecture is kind of clean. There&#8217;s a page for everything that you care about.

The saddest thing unfortunately I have to tell you is that it&#8217;s now offline for review. The classic BBC patent is that if you&#8217;re doing anything major new &#8212; because the BBC isn&#8217;t run by the government, but it sort of follows government charter &#8212; any major new activity has to go through strict process of first time in testing, review, and then a final version. It is looking very likely that they are going to make a even bigger much better version this time. But unfortunately the only way to play with it during this conference is to come up, and talk to me, and I will give you a little preview of the, back up server left online, but only for this conference.

Can we just try that for the recording?

<!-- laughter -->

So I have to say thank you to Flickr, for these fantastic photos, and thank you to you, for listening.

<!-- applause -->

### Audience Questions

Audience Member: One of the challenges with just giving the user a text box like that, is the query string parsing. How much time do you feel out of the entire project did you spend on query string parsing? Because I noticed you typed &#8220;Dr.&#8221; instead of &#8220;doctor&#8221;, and I know challenges like that can be very time consuming.

Matt Biddulph: OK, so the question is, how challenging was it to parse the query string, to parse the input to the search engine, and how much time do I spend on that. Yeah, I spent quite a lot of time on search. It&#8217;s a difficult area; there is always more you can do. So, for example, there is a table in the original BBC database which we did not use because of time constraints. This is the &#8220;see also table&#8221;, so for &#8220;Dr. Who&#8221; see also &#8220;Doctor Who&#8221; and so forth. I think in the end the naive approach was what we took. Essentially to take all the data in the database that had any free text in it, and to match everything in the search box against it, and then spend the effort on the scoring of the results rather than on the parsing.

Audience Member: When you get daily updates from the, what I would call the back end, the librarians, do you only receive new programmes or did you have to update existing data? And could you tell me more about peak levels?

Matt Biddulph: OK, so the question is what degree of daily updates did we receive, diffs, deltas, against the previous day and how do we integrate that with the existing database? So yeah, we work with the team that runs this blue screen, green screen apparatus, some of the original team from the eighties is still working there. They&#8217;re also really fantastic people. This is the most thing about the BBC is all good chaps. Every one is there to do something good.

Matt Biddulph: So what they did for us was they went into their informics database, which runs every thing behind the scenes, and they added new triggers against updates and inserts. So every record that got changed during each day was marked with a dirty flag and at the end of the day they exported only those records. So, basically what the data dump was, was a file per table of every row that was either new or changed. So, again part of this priority queuing stuff is to go through each bit of data and go, &#8220;Have I seen this third-party primary key before, or not?&#8221; and if so integrate or replace the data in the database with that stuff.

Matt Biddulph: So everywhere where I translated something from a legacy table into a new table, I would always keep a record of what the original primary key was. So through out, I did the Rails thing and using auto-increment artificial primary keys. But I always made a record of the original, or natural key. Whatever you&#8217;d want to call it. Also in the URLs everywhere except contributors, which I feel slightly embarrassed about, there is no exposure of any actual internal database primary key. So that&#8217;s another thing I found particularly in previous projects at the BBC is that if your going to expose identifiers to people on the web, that they might actually end up firstly linking to or secondarily incorporating into their own database, as some kind of referencing your data, then using really opaque sort of secondary key that you can lookup and map to your internal primary key.

Matt Biddulph: It&#8217;s very powerful when you&#8217;ve got the kind of data that might get changed internally because you get to maintain that mapping behind. It&#8217;s sort of a identifier firewall for your database. Anyone else?

Audience Member: [inaudible]&#8230;in this project, where was that at?

Matt Biddulph: Well, so, yes, I guess, every bit of XML that I consider the API to the site is a gettable URL. And this when I say rest, this is not as in: &#8220;Oh the API is rest&#8221; that you access later, that you don&#8217;t send it a chunk of XML. This is rest in the academic sense, that everything that this is a resource and representation split in the mentality of the website. Every URL represents a resource, and when you request it using HTTP you get a representation of that resource in the format that you are asked for, whether that is machine readable or human readable. So, a gettable website with XML on it is a rest API. In fact a gettable website with good XHTML on it is a Rest API. Anyone else?

Audience Member: [inaudible]&#8230;your internal key.

Matt Biddulph: Okay, so essentially everywhere where I used my identifier in an URL that I was going to lookup, that was eventually end me up at a row in the database I used something to disguise it, to disguise my large integer from the user. So for example, every programme in the database is given an identifier by the librarians. It&#8217;s like LD1234, so I use that in the URL and use that as the lookup when I pull in a programme. Every tag has a text string so that the tag for ponies is /tag, sorry the URL for ponies is /tag/ponies. So, I think they only re-export their taxonomy every six months, so that&#8217;s going to be a big job re-integrating a taxonomy of new terms. I may actually have to dump and reload the table. But I won&#8217;t break anyone&#8217;s links because all my regenerated auto-increments will be hidden behind the nice opaque identifier. Okay, well should&#8230;

Audience Member: The first time you did the migration of the data&#8230;

Matt Biddulph: Oh yes, well that&#8217;s a good one. So the question is how long did it take to migrate the data? That took about two weeks of full-time CPU.

Audience Member: Was it done in ActiveRecord?

Matt Biddulph: Yes! So everything was done through the front door, through ActiveRecord. First I imported the two-gig of plain-text using MySQL import tools, the commandline tools, to dump that into database tables, then for each of the legacy tables I created a really simple ActiveRecord model. This was just the naive model with no joins or anything in there and a single method, which was called create\_whatever. So for any legacy programme you could call create\_programme and it would return you an ActiveRecord object in the new schema. So after a fifteen minute import of the two gig of data, then followed two-weeks of ActiveRecord churning. Now the nightly imports, which cover from 200 to 1000 programmes that get added or changed, takes about half-hour. That just runs on the live servers.

Audience Member: So in the Mythical Man Month, Brooks talks about how with a surgical development model, a single developer can keep as much of the project as possible in his head at once. This contributes to retaining the coherence of the project. What advantages do you think you had as a independent contractor over working with a whole team?

Matt Biddulph: The question is what advantages did I have as an independent contractor, and as a single person working on the code base as opposed to working with a team? Yeah, I think the classic stuff of being able to keep the whole thing in your head absolutely worked. The thing that I found very empowering about using Rails for this project, after ten-years of all kinds of different frameworks for making websites was that you genuinely could, in some situations, do that good contractor story where you&#8217;d schedule a meeting with the client for the start of the day and you&#8217;d have your one-hour meeting, what do you want to tweak, what do you like about this, what don&#8217;t you like, and then at the end of the day you&#8217;d send them an email going &#8220;On the staging server, your changes are now there.&#8221;

Matt Biddulph: And that&#8217;s a combination of the small number of lines of code that I&#8217;d to tackle to change or add things. The amount of trust I&#8217;d got from my client, which is often forgotten is a really important factor in these things. But also absolutely I could just dive in there. It&#8217;s almost impossible to keep four thousand lines of code in your head, in a kind of block-level structure; you kind of know where to find anything. So you&#8217;d just dive, you could be thinking during the meeting &#8216;Oh yeah! I&#8217;ll change that bit. I can do that.&#8217; or &#8216;Oh, that&#8217;s hard, that&#8217;s a whole new thing&#8217;. So I think it really helped me with actually estimation and time turnaround, being able to just kind of keep the map in my head at all times.

Matt Biddulph: Right, let&#8217;s close it there. But do come and talk to me later. Thank you very much.

<!-- applause -->

 [1]: https://www.hackdiary.com/slides/MattBiddulph_BBCOnRails.pdf
 [2]: https://www.imdb.com/